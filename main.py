# main.py
import asyncio
import json
import os
import sys
from pathlib import Path

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from openai import OpenAI
from dotenv import load_dotenv


load_dotenv()  # .env íŒŒì¼ ë‚´ìš© í™˜ê²½ë³€ìˆ˜ë¡œ ë¡œë“œ



# ğŸ”‘ OpenAI í‚¤
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# MCP ì„œë²„ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ (ê°™ì€ í´ë”ì— mcp_server.py ìˆë‹¤ê³  ê°€ì •)
MCP_SCRIPT = Path(__file__).with_name("mcp_server.py")


SYSTEM_PROMPT = """
ë„ˆëŠ” ì‡¼í•‘ëª° ê³ ê°ì„¼í„° ìƒë‹´ì›ì´ë‹¤.
ì§€ì› ê°€ëŠ¥í•œ ê¸°ëŠ¥:
- ë°°ì†¡ ì¡°íšŒ: ì£¼ë¬¸ë²ˆí˜¸ë¥¼ ì´ìš©í•´ì„œ í˜„ì¬ ë°°ì†¡ ìƒíƒœë¥¼ ì•Œë ¤ì¤„ ìˆ˜ ìˆë‹¤.

ê·œì¹™:
1. ì‚¬ìš©ìê°€ 'ë°°ì†¡', 'íƒë°°', 'ë°°ì†¡ì¡°íšŒ' ê°™ì€ ë§ì„ í•˜ë©´, ë°˜ë“œì‹œ 'track_delivery' ë„êµ¬ë¥¼ í™œìš©í•˜ë ¤ê³  ì‹œë„í•´ë¼.
2. ì£¼ë¬¸ë²ˆí˜¸ë¥¼ ëª¨ë¥´ë©´ ë¨¼ì € ì‚¬ìš©ìì—ê²Œ ì£¼ë¬¸ë²ˆí˜¸ë¥¼ ë¬¼ì–´ë´ë¼.
3. ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ë¥¼ ë°›ìœ¼ë©´, í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ìš”ì•½í•´ì„œ ì•Œë ¤ì¤˜ë¼.
"""


# LLMì—ê²Œ ë…¸ì¶œí•  "tool" ìŠ¤í™ (ì´ toolì„ ì‹¤ì œë¡œëŠ” MCPë¡œ ë¼ìš°íŒ…í•  ê²ƒ)
TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "track_delivery",
            "description": "ì£¼ë¬¸ë²ˆí˜¸(order_id)ë¡œ ë°°ì†¡ ìƒíƒœë¥¼ ì¡°íšŒí•œë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "order_id": {
                        "type": "string",
                        "description": "ì£¼ë¬¸ë²ˆí˜¸, ì˜ˆ: 'ORDER123'",
                    }
                },
                "required": ["order_id"],
            },
        },
    }
]


async def call_mcp_tool(tool_name: str, arguments: dict):
    """
    MCP ì„œë²„(mcp_server.py)ì— stdioë¡œ ë¶™ì–´ì„œ í•´ë‹¹ toolì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜.
    """
    server_params = StdioServerParameters(
        command=sys.executable,
        args=[str(MCP_SCRIPT)],
        env={**os.environ},
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            result = await session.call_tool(tool_name, arguments=arguments)
            return result


def call_llm_with_tools(messages):
    """
    OpenAI LLMì— messages + toolsë¥¼ ë³´ë‚´ì„œ
    - ë„êµ¬ í˜¸ì¶œì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ê²Œ í•˜ê³ 
    - tool_call ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë¦¬í„´.
    """
    resp = client.chat.completions.create(
        model="gpt-5-mini-2025-08-07",  # ë˜ëŠ” ë„¤ê°€ ì“°ëŠ” ëª¨ë¸ëª…
        messages=messages,
        tools=TOOLS,
        tool_choice="auto",
    )
    return resp

def extract_mcp_tool_output(mcp_result) -> str:
    """
    MCP CallToolResultì—ì„œ LLMì— ì¤„ ë¬¸ìì—´ë§Œ ë½‘ì•„ë‚´ëŠ” í—¬í¼.
    """
    # 1) structuredContentê°€ ìˆìœ¼ë©´ ê·¸ê±¸ JSONìœ¼ë¡œ
    if getattr(mcp_result, "structuredContent", None):
        try:
            return json.dumps(mcp_result.structuredContent, ensure_ascii=False)
        except Exception:
            pass

    # 2) content[0].text í˜•ì‹ì´ë©´ ê·¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
    content = getattr(mcp_result, "content", None)
    if content:
        first = content[0]
        if hasattr(first, "text") and first.text is not None:
            return first.text
        # í˜¹ì‹œ dict ê°™ì€ê²Œ ë“¤ì–´ìˆìœ¼ë©´
        try:
            return json.dumps(first, ensure_ascii=False)
        except TypeError:
            return str(first)

    # 3) ìµœí›„ì˜ ìˆ˜ë‹¨: ê·¸ëƒ¥ ë¬¸ìì—´ ë³€í™˜
    return str(mcp_result)


async def chat_once(user_input: str):
    # 1) ìœ ì € ë©”ì‹œì§€ê¹Œì§€ ë„£ê³  1ì°¨ LLM í˜¸ì¶œ
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_input},
    ]

    first = call_llm_with_tools(messages)
    msg = first.choices[0].message

    # 2) LLMì´ tool_callsë¥¼ ìš”ì²­í–ˆëŠ”ì§€ ì²´í¬
    if msg.tool_calls:
        tool_call = msg.tool_calls[0]
        tool_name = tool_call.function.name
        tool_args = json.loads(tool_call.function.arguments or "{}")

        print(f"\nğŸ›  LLMì´ íˆ´ í˜¸ì¶œ ìš”ì²­: {tool_name}({tool_args})")

        # 3) MCP ì„œë²„ì— ì‹¤ì œ íˆ´ í˜¸ì¶œ
        mcp_result = await call_mcp_tool(tool_name, tool_args)
        print(f"ğŸ“¦ MCP íˆ´ ê²°ê³¼(raw): {mcp_result}")

        # 4) íˆ´ ê²°ê³¼ë¥¼ LLMì— ë‹¤ì‹œ ë˜ì ¸ì„œ ìµœì¢… ë‹µë³€ ìƒì„±
        messages.append(
            {
                "role": "assistant",
                "content": None,
                "tool_calls": [tool_call],
            }
        )
        messages.append(
            {
                "role": "tool",
                "tool_call_id": tool_call.id,
                "name": tool_name,
                "content": json.dumps(mcp_result),
            }
        )

        final = client.chat.completions.create(
            model="gpt-5.1-mini",
            messages=messages,
        )
        answer = final.choices[0].message.content
        print(f"\nğŸ’¬ ìµœì¢… ë‹µë³€:\n{answer}\n")

    else:
        # ë„êµ¬ í•„ìš” ì—†ì´ ë°”ë¡œ ë‹µí•œ ê²½ìš°
        print(f"\nğŸ’¬ LLM ì§ì ‘ ë‹µë³€:\n{msg.content}\n")


async def main():
    # í•œ ë²ˆ í…ŒìŠ¤íŠ¸: ì£¼ë¬¸ë²ˆí˜¸ê¹Œì§€ ë‹¤ ë§í•´ì£¼ëŠ” ì¼€ì´ìŠ¤
    print("=== í…ŒìŠ¤íŠ¸ 1: 'ORDER123 ë°°ì†¡ ì¡°íšŒí•´ì¤˜' ===")
    await chat_once("ORDER123 ë°°ì†¡ ì¡°íšŒí•´ì¤˜")

    # í•œ ë²ˆ í…ŒìŠ¤íŠ¸: ì£¼ë¬¸ë²ˆí˜¸ ì—†ì´ â€œë°°ì†¡ì¡°íšŒê°€ ê¶ê¸ˆí•´ìš”â€
    print("\n=== í…ŒìŠ¤íŠ¸ 2: 'ë‚˜ ë°°ì†¡ì¡°íšŒê°€ ê¶ê¸ˆí•´ìš”' ===")
    await chat_once("ë‚˜ ë°°ì†¡ì¡°íšŒê°€ ê¶ê¸ˆí•´ìš”")


if __name__ == "__main__":
    asyncio.run(main())
